"""
Heavy Packet Optimizer
××•×¤×˜×™××™×–×¦×™×” ××ª×§×“××ª ×œ×˜×™×¤×•×œ ×‘×¤×§×˜×•×ª ×›×‘×“×•×ª (1 ×©× ×™×” ×‘-56MHz = 56 ××™×œ×™×•×Ÿ ×¡××¤×œ×™×)
"""

import numpy as np
import psutil
import gc
import warnings
from typing import Optional, Tuple, Union
import time

class HeavyPacketOptimizer:
    """××•×¤×˜×™××™×–×¦×™×” ××ª×§×“××ª ×œ×¤×§×˜×•×ª ×›×‘×“×•×ª"""
    
    def __init__(self):
        self.memory_threshold_gb = 4.0  # ×¡×£ ×–×™×›×¨×•×Ÿ ×‘×’×™×’×”
        self.chunk_size_mb = 100  # ×’×•×“×œ ×—×œ×§ ×‘MB
        self.max_samples_per_chunk = 5_000_000  # ××§×¡×™××•× ×¡××¤×œ×™× ×œ×—×œ×§
        
        # ×–×™×”×•×™ ×–×™×›×¨×•×Ÿ ××¢×¨×›×ª
        self.total_memory_gb = psutil.virtual_memory().total / (1024**3)
        self.available_memory_gb = psutil.virtual_memory().available / (1024**3)
        
        print(f"×–×™×›×¨×•×Ÿ ××¢×¨×›×ª: {self.total_memory_gb:.1f}GB, ×–××™×Ÿ: {self.available_memory_gb:.1f}GB")
        
        # ×”×ª×××ª ×¤×¨××˜×¨×™× ×œ×¤×™ ×–×™×›×¨×•×Ÿ ×–××™×Ÿ
        self._adjust_parameters()
    
    def _adjust_parameters(self):
        """×”×ª×××ª ×¤×¨××˜×¨×™× ××•×˜×•××˜×™×ª ×œ×¤×™ ×–×™×›×¨×•×Ÿ ×–××™×Ÿ"""
        if self.available_memory_gb < 2:
            self.chunk_size_mb = 50
            self.max_samples_per_chunk = 2_000_000
            print("âš ï¸ ×–×™×›×¨×•×Ÿ × ××•×š - ××¦×‘ ×—×¡×›×•× ×™")
        elif self.available_memory_gb > 8:
            self.chunk_size_mb = 200
            self.max_samples_per_chunk = 10_000_000
            print("âœ… ×–×™×›×¨×•×Ÿ ×’×‘×•×” - ××¦×‘ ××”×™×¨")
        else:
            print("âœ… ×–×™×›×¨×•×Ÿ ×¨×’×™×œ - ××¦×‘ ×××•×–×Ÿ")
    
    def estimate_memory_usage(self, signal_length: int, dtype=np.complex64) -> float:
        """×”×¢×¨×›×ª ×©×™××•×© ×‘×–×™×›×¨×•×Ÿ ×‘××’×” ×‘×™×™×˜"""
        bytes_per_sample = np.dtype(dtype).itemsize
        total_mb = (signal_length * bytes_per_sample) / (1024 * 1024)
        
        # ×”×¢×¨×›×” ×›×•×œ×œ×ª ×¢× ×¤×¢×•×œ×•×ª ×¢×™×‘×•×“ (x3 ×‘×’×œ×œ ×¢×•×ª×§×™× ×–×× ×™×™×)
        estimated_mb = total_mb * 3
        return estimated_mb
    
    def should_use_chunking(self, signal_length: int) -> bool:
        """×”×× ×œ×”×©×ª××© ×‘×¢×™×‘×•×“ ××—×•×œ×§ ×œ×—×œ×§×™×"""
        estimated_mb = self.estimate_memory_usage(signal_length)
        return estimated_mb > (self.available_memory_gb * 1024 * 0.5)  # 50% ××”×–×™×›×¨×•×Ÿ ×”×–××™×Ÿ
    
    def optimize_data_type(self, signal: np.ndarray) -> np.ndarray:
        """××•×¤×˜×™××™×–×¦×™×” ×©×œ ×¡×•×’ ×”× ×ª×•× ×™×"""
        if signal.dtype in [np.complex128, np.float64]:
            # ×”××¨×” ×œ-precision × ××•×š ×™×•×ª×¨ ×œ×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ
            if np.iscomplexobj(signal):
                return signal.astype(np.complex64)
            else:
                return signal.astype(np.float32)
        return signal
    
    def chunk_signal(self, signal: np.ndarray, chunk_size: Optional[int] = None) -> list:
        """×—×œ×•×§×ª ×”××•×ª ×œ×—×œ×§×™× ×œ×¢×™×‘×•×“ ×™×¢×™×œ"""
        if chunk_size is None:
            chunk_size = self.max_samples_per_chunk
        
        chunks = []
        for i in range(0, len(signal), chunk_size):
            end_idx = min(i + chunk_size, len(signal))
            chunks.append((i, end_idx, signal[i:end_idx]))
        
        print(f"×—×•×œ×§ ×œ×›-{len(chunks)} ×—×œ×§×™× ×©×œ ×¢×“ {chunk_size:,} ×¡××¤×œ×™×")
        return chunks
    
    def process_heavy_signal(self, signal: np.ndarray, 
                           sample_rate: float,
                           operation: str = "spectrogram",
                           **kwargs) -> tuple:
        """×¢×™×‘×•×“ ××•×ª ×›×‘×“ ×¢× ××•×¤×˜×™××™×–×¦×™×” ××ª×§×“××ª"""
        
        print(f"××¢×‘×“ ××•×ª ×›×‘×“: {len(signal):,} ×¡××¤×œ×™× ({len(signal)/sample_rate:.2f} ×©× ×™×•×ª)")
        
        # ××•×¤×˜×™××™×–×¦×™×” ×¨××©×•× ×™×ª
        signal = self.optimize_data_type(signal)
        
        # ×‘×“×™×§×” ×”×× × ×“×¨×© ×¢×™×‘×•×“ ××—×•×œ×§
        if self.should_use_chunking(len(signal)):
            print("ğŸ”„ ××©×ª××© ×‘×¢×™×‘×•×“ ××—×•×œ×§ ×œ×—×œ×§×™×")
            return self._process_chunked(signal, sample_rate, operation, **kwargs)
        else:
            print("âš¡ ×¢×™×‘×•×“ ×™×©×™×¨")
            return self._process_direct(signal, sample_rate, operation, **kwargs)
    
    def _process_direct(self, signal: np.ndarray, sample_rate: float, 
                       operation: str, **kwargs) -> tuple:
        """×¢×™×‘×•×“ ×™×©×™×¨ ×œ××•×ª×•×ª ×§×˜× ×™× ×™×•×ª×¨"""
        start_time = time.time()
        
        if operation == "spectrogram":
            result = self._create_optimized_spectrogram(signal, sample_rate, **kwargs)
        else:
            raise ValueError(f"×¤×¢×•×œ×” ×œ× × ×ª××›×ª: {operation}")
        
        process_time = time.time() - start_time
        print(f"â±ï¸ ×–××Ÿ ×¢×™×‘×•×“: {process_time:.2f} ×©× ×™×•×ª")
        
        return result
    
    def _process_chunked(self, signal: np.ndarray, sample_rate: float,
                        operation: str, **kwargs) -> tuple:
        """×¢×™×‘×•×“ ××—×•×œ×§ ×œ×—×œ×§×™×"""
        start_time = time.time()
        
        # ×—×œ×•×§×” ×œ×—×œ×§×™× ×¢× ×—×¤×™×¤×”
        overlap_samples = int(sample_rate * 0.01)  # ×—×¤×™×¤×” ×©×œ 10ms
        chunk_size = self.max_samples_per_chunk
        
        chunks_results = []
        total_chunks = (len(signal) + chunk_size - 1) // chunk_size
        
        for i in range(0, len(signal), chunk_size - overlap_samples):
            chunk_num = i // (chunk_size - overlap_samples) + 1
            end_idx = min(i + chunk_size, len(signal))
            chunk = signal[i:end_idx]
            
            print(f"××¢×‘×“ ×—×œ×§ {chunk_num}/{total_chunks} ({len(chunk):,} ×¡××¤×œ×™×)")
            
            # ×¢×™×‘×•×“ ×”×—×œ×§
            if operation == "spectrogram":
                result = self._create_optimized_spectrogram(chunk, sample_rate, **kwargs)
                chunks_results.append((i, result))
            
            # × ×™×§×•×™ ×–×™×›×¨×•×Ÿ
            del chunk
            gc.collect()
            
            if end_idx >= len(signal):
                break
        
        # ××™×—×•×“ ×ª×•×¦××•×ª
        print("ğŸ”— ×××—×“ ×ª×•×¦××•×ª...")
        final_result = self._merge_chunks_results(chunks_results, operation)
        
        process_time = time.time() - start_time
        print(f"â±ï¸ ×–××Ÿ ×¢×™×‘×•×“ ×›×•×œ×œ: {process_time:.2f} ×©× ×™×•×ª")
        
        return final_result
    
    def _create_optimized_spectrogram(self, signal: np.ndarray, sample_rate: float,
                                    max_samples: int = 2_000_000,
                                    time_resolution_us: float = 10.0,
                                    adaptive_resolution: bool = True,
                                    **kwargs) -> tuple:
        """×™×¦×™×¨×ª ×¡×¤×§×˜×¨×•×’×¨××” ××•×ª×××ª ×œ×¤×§×˜×•×ª ×›×‘×“×•×ª"""
        
        from utils import create_spectrogram
        
        # ×”×ª×××ª ×¤×¨××˜×¨×™× ×œ××•×ª ×›×‘×“
        if len(signal) > max_samples:
            # ×“×’×™××” ××—×“×© ××’×¨×¡×™×‘×™×ª ×™×•×ª×¨
            downsample_factor = int(np.ceil(len(signal) / max_samples))
            signal_ds = signal[::downsample_factor]
            fs_ds = sample_rate / downsample_factor
            
            print(f"×“×’×™××” ××—×“×©: ×’×•×¨× {downsample_factor}, ××•×¨×š ×—×“×©: {len(signal_ds):,}")
        else:
            signal_ds = signal
            fs_ds = sample_rate
        
        # ×”×ª×××ª ×¨×–×•×œ×•×¦×™×™×ª ×–××Ÿ ×œ××•×ª ×›×‘×“
        signal_duration_ms = len(signal_ds) / fs_ds * 1000
        if signal_duration_ms > 1000:  # ××¢×œ ×©× ×™×™×”
            time_resolution_us = min(time_resolution_us, 50.0)  # ×œ× ×¤×—×•×ª ×-50Î¼s
        
        return create_spectrogram(
            signal_ds, fs_ds,
            max_samples=max_samples,
            time_resolution_us=int(time_resolution_us),
            adaptive_resolution=adaptive_resolution,
            **kwargs
        )
    
    def _merge_chunks_results(self, chunks_results: list, operation: str) -> tuple:
        """××™×—×•×“ ×ª×•×¦××•×ª ××—×œ×§×™×"""
        if operation == "spectrogram":
            return self._merge_spectrograms(chunks_results)
        else:
            raise ValueError(f"××™×—×•×“ ×œ× × ×ª××š ×¢×‘×•×¨: {operation}")
    
    def _merge_spectrograms(self, chunks_results: list) -> tuple:
        """××™×—×•×“ ×¡×¤×§×˜×¨×•×’×¨××•×ª ××—×œ×§×™×"""
        if not chunks_results:
            raise ValueError("××™×Ÿ ×ª×•×¦××•×ª ×œ××™×—×•×“")
        
        # ×”×©×•×•××ª ×ª×“×¨×™× (×¦×¨×™×›×™× ×œ×”×™×•×ª ×–×”×™×)
        first_freqs = chunks_results[0][1][0]
        
        # ××™×—×•×“ ×–×× ×™× ×•×¡×¤×§×˜×¨×•×’×¨××•×ª
        all_times = []
        all_spectrograms = []
        
        time_offset = 0
        for chunk_idx, (start_sample, (freqs, times, Sxx)) in enumerate(chunks_results):
            # ×”×ª×××ª ×–×× ×™×
            adjusted_times = times + time_offset
            all_times.append(adjusted_times)
            all_spectrograms.append(Sxx)
            
            # ×¢×“×›×•×Ÿ offset ×œ×—×œ×§ ×”×‘×
            if len(adjusted_times) > 0:
                time_offset = adjusted_times[-1] + (adjusted_times[1] - adjusted_times[0] if len(adjusted_times) > 1 else 0)
        
        # ××™×—×•×“ ×¡×•×¤×™
        merged_times = np.concatenate(all_times)
        merged_spectrogram = np.concatenate(all_spectrograms, axis=1)
        
        return first_freqs, merged_times, merged_spectrogram
    
    def monitor_memory_usage(self) -> dict:
        """× ×™×˜×•×¨ ×©×™××•×© ×‘×–×™×›×¨×•×Ÿ"""
        memory = psutil.virtual_memory()
        return {
            'total_gb': memory.total / (1024**3),
            'available_gb': memory.available / (1024**3),
            'used_gb': memory.used / (1024**3),
            'percent': memory.percent
        }
    
    def cleanup_memory(self):
        """× ×™×§×•×™ ×–×™×›×¨×•×Ÿ ××’×¨×¡×™×‘×™"""
        gc.collect()
        memory_after = self.monitor_memory_usage()
        print(f"× ×™×§×•×™ ×–×™×›×¨×•×Ÿ - ×–××™×Ÿ: {memory_after['available_gb']:.1f}GB")


# ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×’×œ×•×‘×œ×™×•×ª
def optimize_for_heavy_packets():
    """××•×¤×˜×™××™×–×¦×™×” ×’×œ×•×‘×œ×™×ª ×œ××¢×¨×›×ª ×œ×¤×§×˜×•×ª ×›×‘×“×•×ª"""
    
    # ×”×’×“×¨×•×ª numpy ××•×ª×××•×ª
    try:
        import os
        # ×”×’×‘×œ×ª ××¡×¤×¨ threads ×©×œ numpy ×œ×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ
        os.environ['OPENBLAS_NUM_THREADS'] = '2'
        os.environ['MKL_NUM_THREADS'] = '2'
        os.environ['OMP_NUM_THREADS'] = '2'
        
        # ××•×¤×˜×™××™×–×¦×™×•×ª ×–×™×›×¨×•×Ÿ
        np.seterr(all='ignore')  # ×”×ª×¢×œ××•×ª ×-warnings ×œ×‘×™×¦×•×¢×™×
        
        print("âœ… ××•×¤×˜×™××™×–×¦×™×•×ª ×’×œ×•×‘×œ×™×•×ª ×”×•×¤×¢×œ×•")
    except Exception as e:
        print(f"âš ï¸ ×©×’×™××” ×‘××•×¤×˜×™××™×–×¦×™×•×ª: {e}")


def estimate_processing_time(signal_length: int, sample_rate: float) -> dict:
    """×”×¢×¨×›×ª ×–×× ×™ ×¢×™×‘×•×“ ×œ×¤×§×˜×•×ª ×›×‘×“×•×ª"""
    
    optimizer = HeavyPacketOptimizer()
    
    # ×”×¢×¨×›×•×ª ×–××Ÿ ×‘×”×ª×‘×¡×¡ ×¢×œ ×’×•×“×œ ×•××™×›×•×ª ××¢×¨×›×ª
    base_time_per_sample = 2e-7  # ×–××Ÿ ×‘×¡×™×¡ ×œ×¡××¤×œ (×©× ×™×•×ª)
    memory_factor = 1.0
    
    if optimizer.should_use_chunking(signal_length):
        memory_factor = 1.5  # ×¢×™×‘×•×“ ××—×•×œ×§ ×œ×•×§×— ×–××Ÿ × ×•×¡×£
    
    estimated_seconds = signal_length * base_time_per_sample * memory_factor
    
    return {
        'signal_length': signal_length,
        'signal_duration_sec': signal_length / sample_rate,
        'estimated_processing_sec': estimated_seconds,
        'needs_chunking': optimizer.should_use_chunking(signal_length),
        'estimated_memory_mb': optimizer.estimate_memory_usage(signal_length)
    }


# ×”×¤×¢×œ×” ××•×˜×•××˜×™×ª ×©×œ ××•×¤×˜×™××™×–×¦×™×•×ª
optimize_for_heavy_packets()